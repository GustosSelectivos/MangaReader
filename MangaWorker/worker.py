import os
import asyncio
import aiohttp
import boto3
from tqdm.asyncio import tqdm
from io import BytesIO

# Configuraci√≥n b√°sica (Idealmente usar venv variables)
# B2_BUCKET_NAME = os.getenv("B2_BUCKET_NAME")
# B2_ENDPOINT = os.getenv("B2_ENDPOINT")
# B2_KEY_ID = os.getenv("B2_KEY_ID")
# B2_APP_KEY = os.getenv("B2_APP_KEY")

async def download_image(session, url, current_index, total, save_dir):
    """Descarga una sola imagen de forma as√≠ncrona y la guarda en disco"""
    try:
        filename = f"{current_index:03d}.webp"
        filepath = os.path.join(save_dir, filename)
        
        async with session.get(url) as response:
            if response.status == 200:
                data = await response.read()
                
                # Guardar en disco localmente (Simulaci√≥n de "Tmp" antes de subir)
                with open(filepath, "wb") as f:
                    f.write(data)
                
                print(f"‚úÖ Guardada {filename} ({current_index}/{total})")
                return filepath
            else:
                print(f"‚ùå Error descargando {url}: Status {response.status}")
                return None
    except Exception as e:
        print(f"‚ùå Exception en {url}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Exception en {url}: {e}")
        return None

async def process_chapter(chapter_url):
    """
    L√≥gica principal:
    1. Obtener HTML del cap√≠tulo
    2. Extraer URLs de im√°genes (Aqu√≠ pondr√°s tu l√≥gica de scraping liviana)
    3. Descargar en paralelo
    """
    print(f"üöÄ Iniciando proceso para: {chapter_url}")
    
    # 1. Obtener HTML del cap√≠tulo
    print(f"üåç Descargando HTML: {chapter_url}")
    
    # Extraer dominio para el Referer
    from urllib.parse import urlparse
    domain = urlparse(chapter_url).netloc
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": f"https://{domain}/"
    }
    
    async with aiohttp.ClientSession(headers=headers) as session:
        async with session.get(chapter_url) as response:
            if response.status != 200:
                print(f"‚ùå Error al acceder al cap√≠tulo: {response.status}")
                return
            html = await response.text()
            print(f"üìç URL Final tras redirecci√≥n: {response.url}")
            
            # TMO/ZonaTMO Specific: Switch to Cascade if stuck on Paginated
            final_url = str(response.url)
            if "/paginated" in final_url:
                print("üîÑ Detectado modo 'Paginated'. Cambiando a 'Cascade' para ver todas las im√°genes...")
                cascade_url = final_url.replace("/paginated", "/cascade")
                
                # Update headers for the new request
                headers["Referer"] = f"https://{domain}/"
                
                async with session.get(cascade_url) as cascade_response:
                    if cascade_response.status == 200:
                        html = await cascade_response.text()
                        print(f"‚úÖ √âxito cambianda a Cascade: {cascade_url}")
                    else:
                        print(f"‚ö†Ô∏è Fall√≥ el cambio a Cascade ({cascade_response.status}). Usando HTML original.")

    from bs4 import BeautifulSoup
    import re
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Olympus suele tener las im√°genes en tags <img> dentro de un contenedor espec√≠fico
    # Buscamos patrones comunes. NOTA: Esto puede variar si el sitio cambia.
    # Estrategia: Buscar todas las im√°genes que parezcan del cap√≠tulo
    images = soup.find_all('img')
    image_urls = []
    
    for img in images:
        src = img.get('src')
        if src and ('uploads' in src or 'storage' in src) and not 'logo' in src:
            image_urls.append(src)
            
    # Si no encontramos nada, puede ser que est√©n en un script JSON (NextJS/React)
    if not image_urls:
         print("‚ö†Ô∏è No se encontraron im√°genes en el HTML est√°tico. Intentando b√∫squeda por Regex...")
         # Intento de regex para buscar URLs de im√°genes comunes
         urls = re.findall(r'(https?://[^"\s]+\.(?:jpg|jpeg|png|webp))', html)
         image_urls = [u for u in urls if 'uploads' in u and 'logo' not in u]

    # Eliminar duplicados manteniendo orden
    image_urls = list(dict.fromkeys(image_urls))
    
    print(f"üîç Encontradas {len(image_urls)} im√°genes.")
    
    total_images = len(image_urls)
    if total_images == 0:
        print("‚ùå No se pudieron extraer im√°genes. Puede que el sitio requiera JS (Selenium/Chromium).")
        return

    # Crear carpeta de descargas base
    base_download_dir = "downloads"
    os.makedirs(base_download_dir, exist_ok=True)
    
    # Buscar siguiente carpeta disponible 001-999
    download_dir = ""
    for i in range(1, 1000):
        folder_name = f"{i:03d}"
        path = os.path.join(base_download_dir, folder_name)
        if not os.path.exists(path):
            download_dir = path
            break
            
    if not download_dir:
        print("‚ùå Error: Se alcanz√≥ el l√≠mite de 999 carpetas en downloads/")
        return

    os.makedirs(download_dir, exist_ok=True)
    print(f"üìÇ Guardando im√°genes en: {os.path.abspath(download_dir)}")

    # 3. Descarga Paralela
    async with aiohttp.ClientSession(headers=headers) as session:
        tasks = []
        for i, url in enumerate(image_urls):
            task = download_image(session, url, i+1, total_images, download_dir)
            tasks.append(task)
        
        await asyncio.gather(*tasks)
        print(f"‚ú® ¬°Descarga completa! Revisa la carpeta '{download_dir}'")
        
        # 4. AUDITOR√çA MANUAL
        print("\n" + "="*50)
        print("üëÄ  MODO AUDITOR√çA  üëÄ")
        print("="*50)
        print("1. Abre la carpeta 'downloads' y BORRA las im√°genes que no quieras (publicidad, etc).")
        print("2. Verifica que el orden sea correcto.")
        
        # Abrir carpeta autom√°ticamente en Windows
        if os.name == 'nt':
            try:
                os.startfile(os.path.abspath(download_dir))
            except:
                pass

        confirm = input("\nüëâ ¬øListo para subir a Backblaze? (Escribe 'S' y Enter): ")
        if confirm.lower() != 's':
            print("‚ùå Proceso cancelado. No se subi√≥ nada.")
            return

        print("\nüöÄ Iniciando subida a Backblaze (Simulada por ahora)...")
        # Aqu√≠ llamar√≠amos a la funci√≥n upload_directory_to_b2(download_dir)
        print("‚ú® ¬°Subida completada! (Mentira, es un print)")

if __name__ == "__main__":
    url_capitulo = "https://zonatmo.com/index.php/view_uploads/1682235"
    try:
        if os.name == 'nt':
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(process_chapter(url_capitulo))
    except ModuleNotFoundError as e:
        print(f"\n‚ùå ERROR CR√çTICO: {e}")
        print("‚ÑπÔ∏è  Est√°s ejecutando esto con el Python equivocado.")
        print("üëâ EJECUTA: ..\\venv\\Scripts\\python.exe worker.py")

